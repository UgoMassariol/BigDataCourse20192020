{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# New York taxis trips\n",
    "\n",
    "This homework is about New York taxi trips. Here is something from [Todd Schneider](https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/):\n",
    "\n",
    "> The New York City Taxi & Limousine Commission has released a  detailed historical dataset covering over 1 billion individual taxi trips in the city from January 2009 through December 2019. \n",
    "Taken as a whole, the detailed trip-level data is more than just a vast list of taxi pickup and drop off coordinates: it's a story of a City. \n",
    "How bad is the rush hour traffic from Midtown to JFK? \n",
    "Where does the Bridge and Tunnel crowd hang out on Saturday nights?\n",
    "What time do investment bankers get to work? How has Uber changed the landscape for taxis?\n",
    "The dataset addresses all of these questions and many more.\n",
    "\n",
    "The NY taxi trips dataset has been plowed by series of distinguished data scientists.\n",
    "The dataset is available from on Amazon S3 (Amazon's cloud storage service).\n",
    "The link for each file has the following form:\n",
    "\n",
    "    https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_{year}-{month}.csv\n",
    "\n",
    "There is one CSV file for each NY taxi service (`yellow`, `green`, `fhv`) and each calendar month (replacing `{year}` and `{month}` by the desired ones).\n",
    "Each file is moderately large, a few gigabytes. \n",
    "The full dataset is relatively large if it has to be handled on a laptop (several hundred gigabytes).\n",
    "\n",
    "You will focus on the `yellow` taxi service and a pair of months, from year 2015 and from year 2018. \n",
    "Between those two years, for hire vehicles services have taken off and carved a huge marketshare.\n",
    "\n",
    "Whatever the framework you use, `CSV` files prove hard to handle. \n",
    "After downloading the appropriate files (this takes time, but this is routine), a first step will consist in converting the csv files into a more Spark friendly format such as `parquet`.\n",
    "\n",
    "Saving into one of those formats require decisions about bucketing, partitioning and so on. Such decisions influence performance. It is your call.\n",
    "Many people have been working on this dataset, to cite but a few:\n",
    "\n",
    "\n",
    "- [1 billion trips with a vengeance](https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/)\n",
    "- [1 billion trips with R and SQL ](http://freerangestats.info/blog/2019/12/22/nyc-taxis-sql)\n",
    "- [1 billion trips with redshift](https://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html)\n",
    "- [nyc-taxi](https://github.com/fmaletski/nyc-taxi-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your internet connection, **download the files** corresponding to **\"yellow\" taxis** for the years 2015 and 2018. Download **at least one month** (the same) for 2015 and 2018, if you can download all of them.\n",
    "\n",
    "**Hint.** The 12 csv for 2015 are about 23GB in total, but the corresponding parquet file, if you can create it for all 12 months, is only about 3GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **might** need the following stuff in order to work with GPS coordinates and to plot things easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:28.124340Z",
     "start_time": "2020-06-07T21:46:25.824847Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install geojson geojsonio geopandas plotly geopy cffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:30.600573Z",
     "start_time": "2020-06-07T21:46:28.132459Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install descartes shapely folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework **we will let you decide on the tools to use** (expected for Spark) and to **find out information all by yourself** (but don't hesitate to ask questions on the `slack` channel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Loading data as parquet files\n",
    "\n",
    "We want to organize the data on a per year and per service basis. \n",
    "We want to end up with one `parquet` file for each year and each taxi service, since parquet is much better than CSV files.\n",
    "\n",
    "**Hint.** Depending on your internet connection and your laptop, you can use only the \"yellow\" service and use one month of 2015 and 2018\n",
    "\n",
    "CSV files can contain corrupted lines. You may have to work in order to perform ETL (Extract-Transform-Load) in order obtain a properly typed data frame.\n",
    "\n",
    "You are invited to proceed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Try to read the CSV file without imposing a schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.729483Z",
     "start_time": "2020-06-07T21:46:30.609282Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, TimeSliderChoropleth\n",
    "import plotly.express as px\n",
    "import calendar\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import *\n",
    "import json\n",
    "\n",
    "# spark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, udf\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "# from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType\n",
    "\n",
    "conf = SparkConf().setAll([ ('spark.app.name','Spark SQL'),\n",
    "                            ('spark.executor.memory', '8g'), \n",
    "                            ('spark.executor.cores', '4'),\n",
    "                            ('spark.driver.cores', '4'),\n",
    "                            ('spark.cores.max', '4'), \n",
    "                            ('spark.driver.memory','8g')])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf = conf)\\\n",
    "                    .getOrCreate()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.735092Z",
     "start_time": "2020-06-07T21:46:36.732083Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = []\n",
    "\n",
    "# periods = ['2015-01', '2015-02', '2018-01', '2018-02']\n",
    "\n",
    "# for time in periods:\n",
    "#     path = f'yellow_tripdata_{time}.csv'\n",
    "#     print(f'Loading {path} ...')\n",
    "#     df.append(spark.read\\\n",
    "#              .format('csv')\\\n",
    "#              .option(\"header\", \"true\")\\\n",
    "#              .option(\"mode\", \"FAILFAST\")\\\n",
    "#              .option(\"inferSchema\", \"true\")\\\n",
    "#              .option(\"sep\", \",\")\\\n",
    "#              .load(path))\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Inspect the inferred schema. Do you agree with Spark's typing decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T08:17:36.605082Z",
     "start_time": "2020-06-05T08:17:36.596187Z"
    }
   },
   "source": [
    "We load the data and see that the timestamp columns are not of type spark Timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.740986Z",
     "start_time": "2020-06-07T21:46:36.737658Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "#     df[i].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Eventually correct the schema and read again the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast the timestamp column into spark Timestamp type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.746638Z",
     "start_time": "2020-06-07T21:46:36.743536Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = []\n",
    "\n",
    "# for time in periods:\n",
    "#     path = f'yellow_tripdata_{time}.csv'\n",
    "#     print(f'Loading {path}, forcing timestamp format...')\n",
    "#     df.append(spark.read\\\n",
    "#              .format('csv')\\\n",
    "#              .option(\"header\", \"true\")\\\n",
    "#              .option(\"mode\", \"FAILFAST\")\\\n",
    "#              .option(\"inferSchema\", \"true\")\\\n",
    "#              .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm\")\\\n",
    "#              .option(\"sep\", \",\")\\\n",
    "#              .load(path))\n",
    "    \n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the schemas of 2015 and 2018 differ by how was recorded the pickup/dropoff location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save the data into parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving into parquet files, we bind by year, and add month, weekday, hour, minute and duration (in seconds) columns that will be usefull for our future calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.752949Z",
     "start_time": "2020-06-07T21:46:36.749634Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "    \n",
    "#     print(f'Adding columns to {periods[i]} data...')\n",
    "    \n",
    "#     df[i] = df[i].withColumn(\"Month\", fn.month(\"tpep_pickup_datetime\"))\\\n",
    "#                  .withColumn(\"Weekday\", fn.dayofweek(\"tpep_pickup_datetime\"))\\\n",
    "#                  .withColumn(\"Hour\", fn.hour(\"tpep_pickup_datetime\"))\\\n",
    "#                  .withColumn(\"Minute\", fn.minute(\"tpep_pickup_datetime\"))\\\n",
    "#                  .withColumn(\"Duration\", fn.unix_timestamp(\"tpep_dropoff_datetime\") -\n",
    "#                                          fn.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:36.760957Z",
     "start_time": "2020-06-07T21:46:36.757436Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Joining data by year\")\n",
    "\n",
    "# df_by_year = [df[0].union(df[1]), df[2].union(df[3])]\n",
    "\n",
    "# years = ['2015', '2018']\n",
    "\n",
    "# for i in range(2):\n",
    "#     parquet_path = f'yellow_{years[i]}.parquet'\n",
    "    \n",
    "#     print(f\"Writing {years[i]} data into parquet...\")\n",
    "    \n",
    "#     df_by_year[i].repartition(2 * 7 * 24, \"Month\", \"Weekday\", \"Hour\")\\\n",
    "#                  .write\\\n",
    "#                  .mode(\"overwrite\")\\\n",
    "#                  .partitionBy(\"Month\", \"Weekday\", \"Hour\")\\\n",
    "#                  .option(\"maxrecordsFile\", 50000)\\\n",
    "#                  .parquet(parquet_path)\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## In the rest of your work, **you will only use the parquet files you created**, not the csv files (don't forget to choose a partitioning column and a number of partitions when creating the parquet files).\n",
    "\n",
    "**Hint.** Don't forget to ask `Spark` to use all the memory and ressources from your computer.\n",
    "\n",
    "**Hint.** Don't foreget that you should specify a partitioning column and a number of partitions when creating the parquet files.\n",
    "\n",
    "**Hint.** Note that the schemas of the 2015 and 2018 data are different...\n",
    "\n",
    "**Hint.** When working on this, ask you and answer to the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:40.904412Z",
     "start_time": "2020-06-07T21:46:36.763851Z"
    }
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "years = ['2015', '2018']\n",
    "center = (40.735, -73.971)\n",
    "\n",
    "for i in range(2):\n",
    "        parquet_path = f'yellow_{years[i]}.parquet'\n",
    "        print(f'Reading {parquet_path}...')\n",
    "        df.append(spark.read\\\n",
    "                       .parquet(parquet_path)\\\n",
    "                       .repartition(2 * 7 * 24, \"Month\", \"Weekday\", \"Hour\"))\n",
    "        \n",
    "df2015, df2018 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## What is the `StorageLevel` of the dataframe after reading the csv files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:40.919379Z",
     "start_time": "2020-06-07T21:46:40.906716Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df[i].storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## What is the number of partitions of the dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:42.348077Z",
     "start_time": "2020-06-07T21:46:40.922161Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df[i].rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Is it possible to tune this number at loading time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading our dataframes, spark gives a partition of cardinal ~15.\n",
    "When are not convinced it is possible to tune the number at loading time so we used `repartition`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Why would we want to modify the number of partitions when creating the parquet files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T09:29:39.911955Z",
     "start_time": "2020-06-05T09:29:39.906574Z"
    }
   },
   "source": [
    "We might want to modify the number of partitions when creating a parquet file so spark can retrieve it more easily when loading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Investigate (at least) one month of data in 2015\n",
    "\n",
    "From now on, you will be using **the parquet files you created for 2015**.\n",
    "\n",
    "We shall visualize several features of taxi traffic during one calendar month\n",
    "in 2015 and the same calendar month in 2018.\n",
    "\n",
    "**Hint.** In order to build appealing graphics, you may stick to `matplotlib + seaborn`, you can use also\n",
    "`plotly`, which is used a lot to build interactive graphics, but you can use whatever you want.\n",
    "\n",
    "The following longitudes and latitudes encompass Newark and JFK airports, Northern Manhattan and Verazzano bridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:46:42.364673Z",
     "start_time": "2020-06-07T21:46:42.350173Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "long_min = -74.10\n",
    "long_max = -73.70\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "query31 = f'''pickup_longitude BETWEEN {long_min} AND {long_max}\n",
    "              AND pickup_latitude BETWEEN {lat_min} AND {lat_max}\n",
    "              AND dropoff_longitude BETWEEN {long_min} AND {long_max}\n",
    "              AND dropoff_latitude BETWEEN {lat_min} AND {lat_max}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using these boundaries, count the number of trips for each count of passenger and make a plot of that.\n",
    "\n",
    "**Hint.** Filter the 2015 data using pickup and dropoff longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:09.944715Z",
     "start_time": "2020-06-07T21:46:42.370238Z"
    }
   },
   "outputs": [],
   "source": [
    "query31 = f'''pickup_longitude BETWEEN {long_min} AND {long_max}\n",
    "              AND pickup_latitude BETWEEN {lat_min} AND {lat_max}\n",
    "              AND dropoff_longitude BETWEEN {long_min} AND {long_max}\n",
    "              AND dropoff_latitude BETWEEN {lat_min} AND {lat_max}'''\n",
    "\n",
    "df31 = df2015.where(query31)\\\n",
    "             .groupBy(\"Month\", \"passenger_count\")\\\n",
    "             .count()\\\n",
    "             .toPandas()\n",
    "\n",
    "df31[\"Month\"] = df31[\"Month\"].apply(lambda i : calendar.month_name[i])\n",
    "\n",
    "fig31 = px.bar(df31,\n",
    "              x = \"passenger_count\",\n",
    "              y = \"count\",\n",
    "              color = \"Month\",\n",
    "              barmode = \"group\")\n",
    "\n",
    "fig31.update_layout(title = \"Number of trips by passenger count, 2015\",\n",
    "                    xaxis = dict(title = \"Number of passengers\",\n",
    "                                 tickvals = list(range(10))),\n",
    "                    yaxis = dict(title = \"Number of trips\"),\n",
    "                    barmode = \"group\")\n",
    "\n",
    "fig31.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Trips with $0$ or larger than $7$ passengers are pretty rare.\n",
    "We suspect these to be outliers. \n",
    "We need to explore these trips further in order order to understand what might be wrong\n",
    "with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## What's special with trips with zero passengers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that the 0 passengers trips can be explained by either human error when entering the number of passenger, or a missing value that was replaced by 0 as there are no NA values in the table.\n",
    "Another explanation could be that some of these taxis drivers were hired to transport items rather than persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:13.369578Z",
     "start_time": "2020-06-07T21:47:09.948046Z"
    }
   },
   "outputs": [],
   "source": [
    "df2015.filter(col(\"passenger_count\")==0)[[\"trip_distance\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## What's special with trips with more than $6$ passengers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that most of these trips, especially those of distance 0 to be errors. The only explanation we could find for trips with more passengers than legally allowed (5) is that children younger than 7 years old are allowed to sir on the lap of someone, which means that technically the maximum number of passenger is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:15.214256Z",
     "start_time": "2020-06-07T21:47:13.372780Z"
    }
   },
   "outputs": [],
   "source": [
    "df2015.filter(col(\"passenger_count\") > 6)[[\"trip_distance\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## What is the largest distance travelled during this month? Is it the first taxi on the moon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:21:28.363580Z",
     "start_time": "2020-06-07T21:21:28.355387Z"
    }
   },
   "source": [
    "Once again, we suspect human error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:28.830803Z",
     "start_time": "2020-06-07T21:47:15.218325Z"
    }
   },
   "outputs": [],
   "source": [
    "df2015.agg({\"trip_distance\" :\"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Plot the distribution of the trip distances (using an histogram for instance) during year 2015. Focus on trips with non-zero trip distance and trip distance less than 30 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:48.976339Z",
     "start_time": "2020-06-07T21:47:28.839050Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits = range(31),\n",
    "                        inputCol = \"trip_distance\",\n",
    "                        outputCol = \"bucket_distance\")\n",
    "\n",
    "w35 = Window.partitionBy(\"Month\")\n",
    "\n",
    "df35 = bucketizer.transform(df2015.filter((col(\"trip_distance\") > 0) & (col(\"trip_distance\") < 30)))\\\n",
    "                                  .groupBy(\"Month\", \"bucket_distance\")\\\n",
    "                                  .count()\\\n",
    "                                  .withColumn(\"freq\", col(\"count\") / fn.sum(col(\"count\")).over(w35))\\\n",
    "                                  .toPandas()\\\n",
    "                                  .sort_values([\"Month\", \"bucket_distance\"])\n",
    "\n",
    "df35[\"Month\"] = df35[\"Month\"].apply(lambda i : calendar.month_name[i])\n",
    "\n",
    "fig35 = px.bar(df35,\n",
    "                x = \"bucket_distance\",\n",
    "                y = \"freq\",\n",
    "                color = \"Month\",\n",
    "                barmode = \"group\")\n",
    "\n",
    "fig35.update_layout(title = \"Distribution of trip distances (<30 miles), 2015\",\n",
    "                    xaxis = dict(title = \"Distance (miles)\",\n",
    "                                 tickvals = list(range(0, 31, 2))),\n",
    "                    yaxis = dict(title = \"Frequence\",\n",
    "                                 tickvals = [i/20 for i in range(8)],\n",
    "                                 ticktext = [f'{5*i}' + '%' for i in range(8)]))\n",
    "\n",
    "fig35.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what Spark does for these computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the explain method or have a look at the [Spark UI](http://localhost:4040/SQL/) to analyze the job. You should be able to assess \n",
    "    - Parsed Logical Plan\n",
    "    - Analyzed Logical Plan\n",
    "    - Optimized Logical Plan\n",
    "    - Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:47:48.994898Z",
     "start_time": "2020-06-07T21:47:48.978316Z"
    }
   },
   "outputs": [],
   "source": [
    "df2015.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Analyzed Logical Plan and Optimized Logical Plan differ? Spot the differences if any. How would a RDBMS proceed with such a query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the physical plan differ from the Optimized Logical Plan? What are the keywords you would not expects in a RDBMS? What is their meaning? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the stages on [Spark UI](http://localhost:4040/stages/stage). How many *stages* are necessary to complete the Spark job? What are the roles of HashAggregate and Exchange hashpartitioning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the physical plan perform shuffle operations? If yes how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are tasks with respect to stages (in Spark language)? How many tasks are your stages made of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now, compute the following and produce relevant plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Break down the trip distance distribution for each day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:05.945683Z",
     "start_time": "2020-06-07T21:47:49.005773Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w312 = Window.partitionBy(\"Month\", \"Weekday\")\n",
    "\n",
    "df312 = bucketizer.transform(df2015.filter((col(\"trip_distance\") > 0) & (col(\"trip_distance\")<30)) \n",
    "                                                )\\\n",
    "                                     .groupBy(\"Month\", \"Weekday\", \"bucket_distance\")\\\n",
    "                                     .count()\\\n",
    "                                     .withColumn(\"freq\", 100 * col(\"count\") /\n",
    "                                                                 fn.sum(col(\"count\")).over(w312))\\\n",
    "                                     .toPandas()\\\n",
    "                                     .sort_values([\"Month\", \"Weekday\", \"bucket_distance\"])\n",
    "\n",
    "df312[\"Weekday\"] = df312[\"Weekday\"].apply(lambda i : calendar.day_name[i-1])\n",
    "df312[\"Month\"] = df312[\"Month\"].apply(lambda i : calendar.month_name[i])\n",
    "\n",
    "fig312 = px.line(df312,\n",
    "                 x = \"bucket_distance\",\n",
    "                 y = \"freq\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Month\")\n",
    "\n",
    "fig312.update_layout(title = \"Distribution of trip distances (<30 miles), Jan. 2015\",\n",
    "                     xaxis_title = \"Distance (in miles)\")\n",
    "\n",
    "fig312.update_xaxes(dtick = 2)\n",
    "fig312.update_yaxes(title = \"Frequence\",\n",
    "                    dtick = 5,\n",
    "                    ticksuffix = \"%\")\n",
    "\n",
    "fig312.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Count the number of distinct pickup location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the number of distinct pickup location in 2018 as locations in 2015 are too accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:21.179563Z",
     "start_time": "2020-06-07T21:48:05.972373Z"
    }
   },
   "outputs": [],
   "source": [
    "df2018.select(\"PULocationID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:22.649020Z",
     "start_time": "2020-06-07T21:48:21.181816Z"
    }
   },
   "outputs": [],
   "source": [
    "districts = gpd.read_file('./districts.geojson')\n",
    "districts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Compute and display tips and profits as a function of the pickup location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2018 data with location ID, compute the right columns and then use a spatial join with the \"official\" geojson we found on https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:35.891883Z",
     "start_time": "2020-06-07T21:48:22.654305Z"
    }
   },
   "outputs": [],
   "source": [
    "df314 = df2018.withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\"))\\\n",
    "              .select(\"tip_amount\", \"profit\", \"PULocationID\")\\\n",
    "              .groupBy(\"PULocationID\")\\\n",
    "              .agg(fn.avg(\"tip_amount\").alias(\"average_tip\"),\n",
    "                   fn.log(fn.count(fn.lit(1))).alias(\"count\"),\n",
    "                   fn.avg(\"profit\").alias(\"average_profit\"))\\\n",
    "              .toPandas()\\\n",
    "              .rename(columns={\"PULocationID\" : \"objectid\"})\n",
    "\n",
    "districts = gpd.read_file('./districts.geojson')\n",
    "districts[\"objectid\"] = districts[\"objectid\"].astype(\"int32\")\n",
    "\n",
    "df314 = districts.merge(df314, on = \"objectid\", how = \"left\")\n",
    "\n",
    "df314 = df314[[\"count\", \"objectid\", \"average_tip\", \"average_profit\"]]\n",
    "\n",
    "df314 = df314.rename(columns={\"objectid\" : \"id\"})\\\n",
    "             .sort_values(\"id\")\n",
    "\n",
    "with open('./districts.geojson') as f:\n",
    "    districts = json.load(f)\n",
    "    \n",
    "for dico in districts[\"features\"]:\n",
    "    dico[\"id\"] = int(dico[\"properties\"][\"objectid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:38.224877Z",
     "start_time": "2020-06-07T21:48:35.897542Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = [40.7, -74],\n",
    "               zoom_start = 10,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=districts,\n",
    "    name='choropleth',\n",
    "    data=df314,\n",
    "    columns=['id', 'average_tip'],\n",
    "    key_on='id',\n",
    "    fill_color = 'RdPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average tip ($)').add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:40.815365Z",
     "start_time": "2020-06-07T21:48:38.227958Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2 = folium.Map(location = [40.7, -74],\n",
    "               zoom_start = 10,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=districts,\n",
    "    name='choropleth',\n",
    "    data=df314,\n",
    "    columns=['id', 'average_profit'],\n",
    "    key_on='id',\n",
    "    fill_color = 'YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average profit ($)').add_to(m2)\n",
    "\n",
    "folium.LayerControl().add_to(m2)\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Investigate one month of trips data in 2015 and 2018\n",
    "\n",
    " Consider one month of trips data from `yellow` taxis for each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Filter and cache/persist the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Assessing seasonalities and looking at time series\n",
    "\n",
    "Compute and plot the following time series indexed by day of the week and hour of day:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### The number of pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:57.248973Z",
     "start_time": "2020-06-07T21:48:40.819710Z"
    }
   },
   "outputs": [],
   "source": [
    "df421 = [df2015.filter(df2015[\"Month\"] == 1)\\\n",
    "               .filter(df2015[\"Duration\"] < 2000),\n",
    "         df2018.filter(df2018[\"Month\"] == 1)]\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    df421[i] = df421[i].withColumn(\"Duration\", col(\"Duration\") * (1/60))\\\n",
    "                       .groupBy(\"Weekday\", \"Hour\")\\\n",
    "                       .agg(fn.count(fn.lit(1)).alias(\"trips\"),\n",
    "                            fn.avg(\"fare_amount\").alias(\"average_fare\"),\n",
    "                            fn.avg(\"Duration\").alias(\"average_duration\"))\\\n",
    "                       .orderBy(\"Weekday\", \"Hour\")\\\n",
    "                       .toPandas()\n",
    "    \n",
    "    df421[i][\"Weekday\"] = df421[i][\"Weekday\"].apply(lambda i : calendar.day_name[i-1])\n",
    "    df421[i][\"Year\"] = int(y)\n",
    "    \n",
    "df421 = df421[0].append(df421[1])\n",
    "\n",
    "fig421 = px.line(df421,\n",
    "                 x = \"Hour\",\n",
    "                 y = \"trips\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig421.update_layout(title = \"Cumulated number of pickups during each weekday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "fig421.update_xaxes(dtick = 1,\n",
    "                    matches = None)\n",
    "fig421.update_yaxes(title = \"Cumulated pickups over Jan.\",\n",
    "                    dtick = 30000,\n",
    "                    matches = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### The average fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:57.587986Z",
     "start_time": "2020-06-07T21:48:57.281077Z"
    }
   },
   "outputs": [],
   "source": [
    "fig422 = px.line(df421,\n",
    "                 x = \"Hour\",\n",
    "                 y = \"average_fare\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig422.update_layout(title = \"Average fare during each weekday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "fig422.update_xaxes(dtick = 1,\n",
    "                    matches = None)\n",
    "fig422.update_yaxes(title = \"Fare (in $)\",\n",
    "                    matches = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### The average trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:48:57.867707Z",
     "start_time": "2020-06-07T21:48:57.591532Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig423 = px.line(df421,\n",
    "                 x = \"Hour\",\n",
    "                 y = \"average_duration\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig423.update_layout(title = \"Average trip duration during each weekday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "fig423.update_xaxes(dtick = 1,\n",
    "                    matches = None)\n",
    "fig423.update_yaxes(title = \"Duration (in minutes)\",\n",
    "                    matches = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Plot the average number of ongoing trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to compute the number of ongoing trips each minute the following way:\n",
    "    For each minute n, we count the pickups from minute 0 to n (included), and then subtract the dropoffs from minute 0 to n-1 (included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:49:33.642360Z",
     "start_time": "2020-06-07T21:48:57.875728Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1 = Window.orderBy(\"date\")\\\n",
    "            .rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "w2 = Window.orderBy(\"date\")\\\n",
    "            .rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "df424 = [df2015.filter(df2015[\"Month\"] == 1)\\\n",
    "               .filter(df2015[\"Duration\"] < 2000),\n",
    "         df2018.filter(df2018[\"Month\"] == 1)]\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    \n",
    "    pickup = df424[i].withColumn(\"date\", fn.date_trunc(\"minute\", \"tpep_pickup_datetime\"))\\\n",
    "                 .groupBy(\"date\")\\\n",
    "                 .count()\\\n",
    "                 .withColumn(\"cum_pickup\", fn.sum(\"count\").over(w1))\\\n",
    "                 .drop(\"count\")\n",
    "\n",
    "    dropoff = df424[i].withColumn(\"date\", fn.date_trunc(\"minute\", \"tpep_dropoff_datetime\"))\\\n",
    "                     .groupBy(\"date\")\\\n",
    "                     .count()\\\n",
    "                     .withColumn(\"cum_dropoff\", fn.sum(\"count\").over(w2))\\\n",
    "                     .drop(\"count\")\n",
    "    \n",
    "    df424[i] = pickup.join(dropoff, \"date\", \"inner\")\\\n",
    "                    .withColumn(\"time\", fn.split(\"date\", \" \")[1])\\\n",
    "                    .withColumn(\"date\", fn.split(\"date\", \" \")[0])\\\n",
    "                    .withColumn(\"diff\", col(\"cum_pickup\") - col(\"cum_dropoff\"))\\\n",
    "                    .na.fill(0)\\\n",
    "                    .drop(\"cum_pickup\", \"cum_dropoff\")\\\n",
    "                    .groupBy(fn.dayofweek(\"date\").alias(\"Weekday\"), \"time\")\\\n",
    "                    .agg(fn.avg(\"diff\").alias(\"avg_ong\"))\\\n",
    "                    .orderBy(\"Weekday\", \"time\")\\\n",
    "                    .toPandas()\n",
    "    \n",
    "    df424[i][\"Weekday\"] = df424[i][\"Weekday\"].apply(lambda i : calendar.day_name[i-1])\n",
    "    df424[i][\"Year\"] = int(y)\n",
    "    \n",
    "df424 = df424[0].append(df424[1])\n",
    "\n",
    "fig424 = px.line(df424,\n",
    "                 x = \"time\",\n",
    "                 y = \"avg_ong\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig424.update_layout(title = \"Average number of ongoing trips during each weakday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "\n",
    "fig424.update_xaxes(dtick = 60)\n",
    "fig424.update_yaxes(title = \"Number of ongoing trips\",\n",
    "                    dtick = 1000,\n",
    "                    matches = None)\n",
    "\n",
    "fig424.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Rides to the airports\n",
    "\n",
    "In order to find the longitude and lattitude of JFK and Newark airport as well as the longitude and magnitudes \n",
    "of Manhattan, you can use a service like [geojson.io](http://geojson.io/).\n",
    "Plot the following time series, indexed by the day of the week and hour of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Median duration of taxi trip leaving Midtown (Southern Manhattan) headed for JFK Airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first filter the data using a rough estimate of the districts (with a box large enough to contain them), the data is then small enough to be handle with (geo)pandas dataframes where we can apply an exact filter, using (multi)polygons from a geojson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:49:33.776436Z",
     "start_time": "2020-06-07T21:49:33.645275Z"
    }
   },
   "outputs": [],
   "source": [
    "midtown_box = [-74.0091, -73.9587, 40.7269, 40.7729]\n",
    "jfk_box = [-73.8196, -73.7704, 40.6389, 40.6652]\n",
    "\n",
    "midtown = gpd.read_file('./midtown.geojson').geometry[0]\n",
    "jfk = gpd.read_file('./jfk.geojson').geometry[0]\n",
    "m_to_j2015 = udf(lambda a, b, c, d : Point(a, b).within(midtown) and Point(c, d).within(jfk))\n",
    "j_to_m2015 = udf(lambda a, b, c, d : Point(a, b).within(jfk) and Point(c, d).within(midtown))\n",
    "\n",
    "midtown_by_id = [224, 107, 234, 90, 68, 246, 137, 170, 164, 186, 233, 100,229, 162, 161, 230, 48, 50, 163]\n",
    "m_to_j2018 = udf(lambda i, j : (i.isin(midtown_by_id)) and (j == 132))\n",
    "\n",
    "def query2015(a,b):\n",
    "    query = f'''pickup_longitude BETWEEN {a[0]} AND {a[1]}\n",
    "            AND pickup_latitude BETWEEN {a[2]} AND {a[3]}\n",
    "            AND dropoff_longitude BETWEEN {b[0]} AND {b[1]}\n",
    "            AND dropoff_latitude BETWEEN {b[2]} AND {b[3]}'''\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:49:59.924534Z",
     "start_time": "2020-06-07T21:49:33.778721Z"
    }
   },
   "outputs": [],
   "source": [
    "df431 = [df2015.filter(df2015[\"Month\"] == 1),\n",
    "         df2018.filter(df2018[\"Month\"] == 1)]\n",
    "\n",
    "df431[0] = df431[0].where(query2015(midtown_box, jfk_box))\\\n",
    "                         .withColumn(\"to_keep\", m_to_j2015(\"pickup_longitude\",\n",
    "                                                           \"pickup_latitude\",\n",
    "                                                           \"dropoff_longitude\",\n",
    "                                                           \"dropoff_latitude\"))\n",
    "df431[0] = df431[0].filter(col(\"to_keep\") == True)\n",
    "\n",
    "df431[1] = df431[1].filter((col(\"PULocationID\").isin(midtown_by_id)) & (col(\"DOLocationID\") == 132))\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    \n",
    "    df431[i] = df431[i].withColumn(\"Duration\", col(\"Duration\") * (1/60))\\\n",
    "                       .groupBy(\"Weekday\", \"Hour\")\\\n",
    "                       .agg(fn.expr('percentile_approx(Duration, 0.5)').alias(\"Median\"))\\\n",
    "                               .orderBy(\"Weekday\", \"Hour\")\\\n",
    "                               .toPandas()\n",
    "    \n",
    "    df431[i][\"Weekday\"] = df431[i][\"Weekday\"].apply(lambda i : calendar.day_name[i-1])\n",
    "    df431[i][\"Year\"] = int(y)\n",
    "    \n",
    "df431 = df431[0].append(df431[1])\n",
    "\n",
    "fig431 = px.line(df431,\n",
    "                 x = \"Hour\",\n",
    "                 y = \"Median\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig431.update_layout(title = \"Median trip duration from Midtown to JFK during each weekday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "\n",
    "fig431.update_xaxes(dtick = 1)\n",
    "fig431.update_yaxes(title = \"Duration (in minutes)\",\n",
    "                    matches = None)\n",
    "\n",
    "fig431.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Median taxi duration of trip leaving from JFK Airport to Midtown (Southern Manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replicate the previous process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:50:26.500245Z",
     "start_time": "2020-06-07T21:49:59.936442Z"
    }
   },
   "outputs": [],
   "source": [
    "df432 = [df2015.filter(df2015[\"Month\"] == 1),\n",
    "         df2018.filter(df2018[\"Month\"] == 1)]\n",
    "\n",
    "df432[0] = df432[0].where(query2015(jfk_box, midtown_box))\\\n",
    "                         .withColumn(\"to_keep\", j_to_m2015(\"pickup_longitude\",\n",
    "                                                           \"pickup_latitude\",\n",
    "                                                           \"dropoff_longitude\",\n",
    "                                                           \"dropoff_latitude\"))\n",
    "df432[0] = df432[0].filter(col(\"to_keep\") == True)\n",
    "\n",
    "df432[1] = df432[1].filter((col(\"DOLocationID\").isin(midtown_by_id)) & (col(\"PULocationID\") == 132))\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    \n",
    "    df432[i] = df432[i].withColumn(\"Duration\", col(\"Duration\") * (1/60))\\\n",
    "                       .groupBy(\"Weekday\", \"Hour\")\\\n",
    "                       .agg(fn.expr('percentile_approx(Duration, 0.5)').alias(\"Median\"))\\\n",
    "                       .orderBy(\"Weekday\", \"Hour\")\\\n",
    "                       .toPandas()\n",
    "    \n",
    "    df432[i][\"Weekday\"] = df432[i][\"Weekday\"].apply(lambda i : calendar.day_name[i-1])\n",
    "    df432[i][\"Year\"] = int(y)\n",
    "    \n",
    "df432 = df432[0].append(df432[1])\n",
    "\n",
    "fig432 = px.line(df432,\n",
    "                 x = \"Hour\",\n",
    "                 y = \"Median\",\n",
    "                 color = \"Weekday\",\n",
    "                 facet_row = \"Year\")\n",
    "\n",
    "fig432.update_layout(title = \"Median trip duration from JFK to Midtown during each weekday in January\",\n",
    "                     xaxis_title = \"Time of the day\")\n",
    "\n",
    "fig432.update_xaxes(dtick = 1)\n",
    "fig432.update_yaxes(title = \"Duration (in minutes)\",\n",
    "                    matches = None)\n",
    "\n",
    "fig432.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Geographic information\n",
    "\n",
    "For this, you will need to find tools to display maps and to build choropleth maps.\n",
    "We let you look and find relevant tools to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T21:48:36.627879Z",
     "start_time": "2020-06-05T21:48:36.621726Z"
    }
   },
   "source": [
    "### Build a heatmap where color is a function of\n",
    "    1. number of `pickups`\n",
    "    2. number of `dropoffs`\n",
    "    3. number of `pickups` with dropoff at some airport (JFK, LaGuardia, Newark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, a rough filter to work with data of reasonable size, as folium needs a list of coordinates to build a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:52:03.881494Z",
     "start_time": "2020-06-07T21:50:26.503659Z"
    }
   },
   "outputs": [],
   "source": [
    "long_min = -74.3\n",
    "long_max = -73.7\n",
    "lat_min = 40.5\n",
    "lat_max = 40.9\n",
    "\n",
    "loc = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "query441 = f'''{loc[0]} BETWEEN {long_min} AND {long_max}\n",
    "               AND {loc[1]} BETWEEN {lat_min} AND {lat_max}\n",
    "               AND {loc[2]} BETWEEN {long_min} AND {long_max}\n",
    "               AND {loc[3]} BETWEEN {lat_min} AND {lat_max}'''\n",
    "\n",
    "df441 = df2015.where(query441)\\\n",
    "              .select(*loc)\n",
    "\n",
    "for loc in loc:\n",
    "    df441 = df441.withColumn(loc, fn.round(col(loc), 4))\n",
    "    \n",
    "df4411 = df441.select(\"pickup_latitude\", \"pickup_longitude\")\\\n",
    "              .groupBy(\"pickup_latitude\", \"pickup_longitude\")\\\n",
    "              .count()\n",
    "\n",
    "df4412 = df441.select(\"dropoff_latitude\", \"dropoff_longitude\")\\\n",
    "              .groupBy(\"dropoff_latitude\", \"dropoff_longitude\")\\\n",
    "              .count()\n",
    "\n",
    "df4411 = [list(row) for row in df4411.collect()]\n",
    "df4412 = [list(row) for row in df4412.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:52:13.743586Z",
     "start_time": "2020-06-07T21:52:03.893041Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = list(center),\n",
    "               zoom_start = 11,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "HeatMap(df4411,\n",
    "        radius = 1.95,\n",
    "        blur = 2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:52:31.885449Z",
     "start_time": "2020-06-07T21:52:13.745551Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = list(center),\n",
    "               zoom_start = 11,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "HeatMap(df4412,\n",
    "        radius = 2,\n",
    "        blur = 2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process, rough filter by emboxing airports then using geojson polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:57:21.242780Z",
     "start_time": "2020-06-07T21:52:31.895912Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nwbox = [-74.20, -74.14, 40.67, 40.71]\n",
    "jfkbox = [-73.819, -73.770, 40.638, 40.665]\n",
    "lgbox = [-73.89, -73.85, 40.76, 40.78]\n",
    "\n",
    "airports_box = [nwbox, jfkbox, lgbox]\n",
    "query = [f'''dropoff_longitude BETWEEN {box[0]} AND {box[1]}\n",
    "            AND dropoff_latitude BETWEEN {box[2]} AND {box[3]}''' for box in airports_box]\n",
    "\n",
    "districts = gpd.read_file('./districts.geojson')\n",
    "airports_geom2 = list(districts[districts[\"objectid\"].isin(['1', '132', '138'])][\"geometry\"])\n",
    "# airports_geom = [gpd.read_file(f'./{x}.geojson').geometry[0] for x in ['nw', 'jfk', 'lg']]\n",
    "\n",
    "df = []\n",
    "\n",
    "for i in range(3):\n",
    "    temp = df441.where(query[i])\\\n",
    "                .withColumn(\"to_keep\",\n",
    "                            udf(lambda a, b : Point(a,b).within(airports_geom2[i]), BooleanType())(\"dropoff_longitude\", \"dropoff_latitude\"))\\\n",
    "                .filter(col(\"to_keep\") == True)\\\n",
    "                .select(\"pickup_latitude\", \"pickup_longitude\")\n",
    "    df.append([list(row) for row in temp.collect()])\n",
    "    \n",
    "df4413 = df[0] + df[1] + df[2]\n",
    "\n",
    "m = folium.Map(location = list(center),\n",
    "               zoom_start = 11,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "HeatMap(df4413,\n",
    "        radius = 2,\n",
    "        blur = 2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a choropleth map where color is a function of\n",
    "    1. number of pickups in the area\n",
    "    2. ratio of number of payments by card/number of cash payments for pickups in the area\n",
    "    3. ratio of total fare/trip duration for dropoff in the area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already computed the column needed here, in question 3.14, thus we use df314."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:57:25.061226Z",
     "start_time": "2020-06-07T21:57:21.256743Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = [40.7, -74],\n",
    "               zoom_start = 10,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=districts,\n",
    "    name='choropleth',\n",
    "    data=df314,\n",
    "    columns=['id', 'count'],\n",
    "    key_on='id',\n",
    "    fill_color = 'RdPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Number of pickups').add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-07T20:37:59.012Z"
    }
   },
   "source": [
    "We couldn't find on time which payment the numbers referred to, thus we are not able to produce the choropleth map for card/cash ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:57:43.994691Z",
     "start_time": "2020-06-07T21:57:25.065540Z"
    }
   },
   "outputs": [],
   "source": [
    "df4423 = df2018.withColumn(\"Duration\", col(\"Duration\") * (1/60))\\\n",
    "               .withColumn(\"minute_cost\", col(\"total_amount\") / col(\"Duration\"))\\\n",
    "               .groupBy(\"DOLocationID\")\\\n",
    "               .agg(fn.avg(\"minute_cost\").alias(\"average_minute_cost\"))\\\n",
    "               .toPandas()\\\n",
    "               .rename(columns={\"DOLocationID\" : \"objectid\"})   \n",
    "\n",
    "districts = gpd.read_file('./districts.geojson')\n",
    "districts[\"objectid\"] = districts[\"objectid\"].astype(\"int32\")\n",
    "\n",
    "df4423 = districts.merge(df4423, on = \"objectid\", how = \"left\")\n",
    "\n",
    "df4423 = df4423[[\"objectid\", \"average_minute_cost\"]]\n",
    "\n",
    "df4423 = df4423.rename(columns={\"objectid\" : \"id\"})\\\n",
    "               .sort_values(\"id\")\n",
    "\n",
    "with open('./districts.geojson') as f:\n",
    "    districts = json.load(f)\n",
    "    \n",
    "for dico in districts[\"features\"]:\n",
    "    dico[\"id\"] = int(dico[\"properties\"][\"objectid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:57:46.375916Z",
     "start_time": "2020-06-07T21:57:43.997050Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = [40.7, -74],\n",
    "               zoom_start = 10,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=districts,\n",
    "    name='choropleth',\n",
    "    data=df4423,\n",
    "    columns=['id', 'average_minute_cost'],\n",
    "    key_on='id',\n",
    "    fill_color = 'RdPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average minute cost (in $)').add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an interactive chorophlet with a slider allowing the user to select an hour of day and where the color is a function of\n",
    "    1. average number of dropoffs in the area during that hour the day\n",
    "    2. average ratio of tip over total fare amount for pickups in the area at given hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:58:15.492397Z",
     "start_time": "2020-06-07T21:57:46.378881Z"
    }
   },
   "outputs": [],
   "source": [
    "df4431 = df2018.groupBy(\"Month\", \n",
    "                        fn.dayofmonth(\"tpep_pickup_datetime\"), \n",
    "                        \"Hour\", \n",
    "                        \"DOLocationID\")\\\n",
    "               .count()\\\n",
    "               .groupBy(\"Hour\", \"DOLocationID\")\\\n",
    "               .agg(fn.avg(\"count\").alias(\"average_dropoffs\"))\\\n",
    "               .toPandas()\\\n",
    "               .rename(columns={\"DOLocationID\" : \"objectid\"})\n",
    "\n",
    "districts = gpd.read_file('./districts.geojson')\n",
    "districts[\"objectid\"] = districts[\"objectid\"].astype(\"int32\")\n",
    "\n",
    "df4431 = districts.merge(df4431, on = \"objectid\", how = \"left\")\n",
    "\n",
    "df4431 = df4431.rename(columns={\"objectid\" : \"id\"})\\\n",
    "               .sort_values(\"id\")\n",
    "\n",
    "with open('./districts.geojson') as f:\n",
    "    districts = json.load(f)\n",
    "    \n",
    "for dico in districts[\"features\"]:\n",
    "    dico[\"id\"] = int(dico[\"properties\"][\"objectid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T21:59:05.958839Z",
     "start_time": "2020-06-07T21:59:03.614116Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = [40.7, -74],\n",
    "               zoom_start = 10,\n",
    "               tiles = 'https://tiles.stadiamaps.com/tiles/alidade_smooth_dark/{z}/{x}/{y}{r}.png',\n",
    "               attr = '&copy; <a href=\"https://stadiamaps.com/\">Stadia Maps</a>, &copy; <a href=\"https://openmaptiles.org/\">OpenMapTiles</a> &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=districts,\n",
    "    name='choropleth',\n",
    "    data=df4431,\n",
    "    columns=['id', 'average_dropoffs'],\n",
    "    key_on='id',\n",
    "    fill_color = 'RdPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average minute cost (in $)').add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We couldn't understand on time how to use the TimeSliderChoropleth plugin of folium, thus not producing the asked map."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
